# System Architecture - Bike Sharing Data Platform

## Component Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Monthly Trip CSVs                                                   â”‚
â”‚  â€¢ ride_id                                                           â”‚
â”‚  â€¢ rideable_type                                                     â”‚
â”‚  â€¢ started_at / ended_at                                             â”‚
â”‚  â€¢ start/end station_id                                              â”‚
â”‚  â€¢ ~800K trips/month                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SPARK ETL         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Read CSVs         â”‚
â”‚ â€¢ Convert to NYC TZ â”‚
â”‚ â€¢ Floor to hour     â”‚
â”‚ â€¢ Aggregate deltas  â”‚
â”‚ â€¢ Split by type     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SQLITE DATABASE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚  station     â”‚   â”‚  events_hourly    â”‚                          â”‚
â”‚  â”‚              â”‚   â”‚                   â”‚                          â”‚
â”‚  â”‚ â€¢ id (PK)    â”‚   â”‚ â€¢ hour (PK)       â”‚                          â”‚
â”‚  â”‚ â€¢ name       â”‚   â”‚ â€¢ station_id (PK) â”‚                          â”‚
â”‚  â”‚ â€¢ lat/lon    â”‚   â”‚ â€¢ delta_total     â”‚                          â”‚
â”‚  â”‚ â€¢ capacity   â”‚   â”‚ â€¢ delta_ebike     â”‚                          â”‚
â”‚  â”‚ â€¢ is_active  â”‚   â”‚ â€¢ delta_other     â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚         â”‚                     â”‚                                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                                    â”‚
â”‚  Database Layer (database/repositories.py):                        â”‚
â”‚  â€¢ StationRepository           - CRUD for stations                 â”‚
â”‚  â€¢ EventsHourlyRepository      - CRUD + balance queries            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STREAMLIT DASHBOARD                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ Time Simulation    â”‚  â”‚ Balance Monitor  â”‚                      â”‚
â”‚  â”‚                    â”‚  â”‚                  â”‚                      â”‚
â”‚  â”‚ â€¢ Current hour     â”‚  â”‚ â€¢ Top 10 low     â”‚                      â”‚
â”‚  â”‚ â€¢ +1 hour          â”‚  â”‚ â€¢ Top 10 high    â”‚                      â”‚
â”‚  â”‚ â€¢ +6 hours         â”‚  â”‚ â€¢ Warnings       â”‚                      â”‚
â”‚  â”‚ â€¢ Reset            â”‚  â”‚ â€¢ Color coding   â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Manual Rebalancing                                           â”‚  â”‚
â”‚  â”‚ â€¢ Click to select stations                                   â”‚  â”‚
â”‚  â”‚ â€¢ Adjust bike counts (+/-)                                   â”‚  â”‚
â”‚  â”‚ â€¢ Apply adjustments (persistent)                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow Details

### Batch ETL (Historical Data)

```
Trip CSV â†’ Spark Read â†’ Transform â†’ Aggregate â†’ Write to SQLite â†’ Streamlit Visualization
```

**Transform steps:**
1. Parse timestamps: `started_at`, `ended_at`
2. Convert to Chicago timezone (`America/Chicago` - Central Time)
3. Floor to hour: `2025-07-15 14:37:22` â†’ `2025-07-15 14:00:00`
4. Create delta events:
   - Start: `(start_hour, start_station_id, -1)`
   - End: `(end_hour, end_station_id, +1)`
5. Split by `rideable_type`:
   - `electric_bike` â†’ `delta_ebike`
   - Others â†’ `delta_other`
6. Aggregate: `GROUP BY (hour, station_id)`

**Output schema:**
```python
{
    'hour': '2025-07-15 14:00:00',
    'station_id': 'CHI00123',
    'delta_total': -5,      # 5 more departures than arrivals
    'delta_ebike': -2,      # 2 from ebikes
    'delta_other': -3       # 3 from classic bikes
}
```

### Dashboard Queries

**Balance calculation:**
```sql
SELECT 
    s.id as station_id,
    s.name as station_name,
    COALESCE(SUM(e.delta_total), 0) as total_delta
FROM station s
LEFT JOIN events_hourly e 
    ON s.id = e.station_id 
    AND e.hour <= '2025-07-15 14:00:00'
WHERE s.is_active = 1
GROUP BY s.id, s.name

-- Balance = INITIAL_BALANCE (20) + total_delta + manual_adjustments
```

## Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Data Source** | Divvy Trip CSVs | Raw bike-sharing trip data |
| **ETL** | Apache Spark (PySpark) | Distributed batch processing |
| **Storage** | SQLite 3 | Fast, embedded database |
| **Data Access** | Python (repositories.py) | Type-safe CRUD operations |
| **Visualization** | Streamlit | Interactive web dashboard |
| **Deployment** | Docker Compose | Containerized services |

## Database Schema Details

### `station` (Dimension Table)
- **Purpose**: Station catalog and metadata
- **Source**: Derived from trip CSVs (station_id, station_name from trips)
- **Grain**: One row per station
- **Updates**: Upsert on station ID (name/location may change)

### `events_hourly` (Fact Table)
- **Purpose**: Hourly bike movement aggregates
- **Source**: Spark ETL from trip histories
- **Grain**: One row per (hour, station_id)
- **Updates**: Upsert allows reprocessing (idempotent)
- **Key insight**: Net change, not absolute counts

## Separation of Concerns

### Tun (Database Layer) âœ…
- **Owns**: `database/`, `config/database.py`
- **Provides**: Clean API for data access
- **Guarantees**: Schema stability, type safety
- **Contract**: Repository methods, timezone conventions

### Sebi (Spark ETL) ğŸš§
- **Owns**: Spark pipeline code (future: `etl/`)
- **Consumes**: Trip CSVs
- **Produces**: Writes to `events_hourly`, `station` via repositories
- **Contract**: NYC timezone, hourly grain, delta semantics

### Felix (Frontend) ğŸš§
- **Owns**: `src/streamlit_test.py`
- **Consumes**: Reads via repositories
- **Produces**: User interactions, manual adjustments
- **Contract**: Uses provided query methods

### Oli (DevOps) ğŸš§
- **Owns**: `docker-compose.yml`, Dockerfile
- **Provides**: Shared volume for `app.db`
- **Ensures**: Service orchestration, health checks

## Key Design Decisions

### Why SQLite?
- âœ… Zero-configuration, embedded
- âœ… ACID transactions
- âœ… Fast for read-heavy workloads
- âœ… Sufficient for current scale (~700 stations, hourly grain)
- âš ï¸ Single writer (acceptable for batch-only workflow)
- ğŸ”„ Can migrate to Postgres later with minimal code changes

### Why Repository Pattern?
- âœ… Decouples business logic from SQL
- âœ… Type-safe interfaces
- âœ… Easier testing (can mock repositories)
- âœ… Centralizes query logic
- âœ… Reduces SQL injection risks

### Why Hourly Grain?
- âœ… Balances detail vs. data volume
- âœ… Matches operational decision-making (rebalancing hourly)
- âœ… Aggregatable to daily/weekly for trends
- âœ… Keeps SQLite database small

### Why Delta (not Snapshot)?
- âœ… Smaller storage (events, not full state every hour)
- âœ… Supports arbitrary time ranges (sum deltas)
- âœ… Rebalancing-friendly (adjustments are additive)
- âœ… Naturally handles missing hours (no implicit zeros)

## Workflow

1. **Data Preparation**: Download monthly CSV files from Divvy
2. **ETL Processing**: Spark reads CSVs, transforms, aggregates to hourly deltas
3. **Database Load**: Spark writes to SQLite via repositories
4. **Visualization**: Streamlit reads from SQLite and displays interactive dashboard
5. **Analysis**: Users simulate time progression, identify imbalances, record adjustments

---

**Document Version**: 1.0  
**Last Updated**: October 14, 2025  
**Maintained by**: Tun (Database Layer)
