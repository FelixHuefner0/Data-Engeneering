{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Divvy Bike Sharing Data Platform - Complete Pipeline\n",
        "\n",
        "**Team:** Felix, Tun, Sebi, Oli  \n",
        "**Course:** Data Engineering  \n",
        "**Date:** October 2025\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates the complete end-to-end pipeline for the Divvy bike-sharing analytics platform:\n",
        "\n",
        "1. **Setup & Configuration** - Initialize environment and dependencies\n",
        "2. **Database Initialization** - Create SQLite schema\n",
        "3. **Data Exploration** - Load and explore raw CSV trip data\n",
        "4. **ETL Pipeline** - Extract stations and transform to hourly deltas using PySpark\n",
        "5. **Data Loading** - Write processed data to SQLite database\n",
        "6. **Analysis & Visualization** - Query results and create insights\n",
        "7. **Results** - Station balances, imbalance detection, and operational metrics\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "CSV Trip Data â†’ PySpark ETL â†’ SQLite Database â†’ Analysis & Visualization\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration\n",
        "\n",
        "Import required libraries and configure project paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Project Root: /Users/tunkeltesch/Desktop/Uni-Repos/Data-Engeneering\n",
            "âœ“ Database Path: /Users/tunkeltesch/Desktop/Uni-Repos/Data-Engeneering/data/app.db\n",
            "âœ“ Timezone: America/Chicago\n",
            "âœ“ Initial Balance: 20 bikes per station\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import sqlite3\n",
        "\n",
        "# Add project root to path\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "# Import PySpark\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
        "\n",
        "# Import project modules\n",
        "from database.connection import DatabaseConnection\n",
        "from database.repositories import StationRepository, EventsHourlyRepository\n",
        "from config.database import DB_PATH, TIMEZONE, INITIAL_BALANCE\n",
        "\n",
        "print(f\"âœ“ Project Root: {PROJECT_ROOT}\")\n",
        "print(f\"âœ“ Database Path: {DB_PATH}\")\n",
        "print(f\"âœ“ Timezone: {TIMEZONE}\")\n",
        "print(f\"âœ“ Initial Balance: {INITIAL_BALANCE} bikes per station\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Database Initialization\n",
        "\n",
        "Create the SQLite database schema with two tables:\n",
        "- **station**: Station catalog (id, name, location, capacity)\n",
        "- **events_hourly**: Hourly deltas per station (hour, station_id, delta_total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Database initialized: /Users/tunkeltesch/Desktop/Uni-Repos/Data-Engeneering/data/app.db\n",
            "âœ“ Database initialized successfully\n",
            "  Tables created: station, events_hourly\n"
          ]
        }
      ],
      "source": [
        "# Initialize database\n",
        "db = DatabaseConnection(DB_PATH)\n",
        "db.initialize_schema()\n",
        "\n",
        "# Verify tables were created\n",
        "with db.get_connection() as conn:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
        "    tables = [row[0] for row in cursor.fetchall()]\n",
        "    \n",
        "print(\"âœ“ Database initialized successfully\")\n",
        "print(f\"  Tables created: {', '.join(tables)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize PySpark\n",
        "\n",
        "Create Spark session configured for Chicago timezone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/11/02 17:25:39 WARN Utils: Your hostname, SM1LE-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)\n",
            "25/11/02 17:25:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/11/02 17:25:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Spark session created\n",
            "  Version: 4.0.1\n",
            "  Timezone: America/Chicago\n"
          ]
        }
      ],
      "source": [
        "# Build Spark session\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"DivvyBikeSharingETL\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
        "    .config(\"spark.sql.session.timeZone\", TIMEZONE)\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(f\"âœ“ Spark session created\")\n",
        "print(f\"  Version: {spark.version}\")\n",
        "print(f\"  Timezone: {TIMEZONE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Raw Trip Data\n",
        "\n",
        "Load Divvy trip CSV files from `data/tripdata/` directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/02 17:25:48 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /Users/tunkeltesch/Desktop/Uni-Repos/Data-Engeneering/data/tripdata/*.csv.\n",
            "java.io.FileNotFoundException: File /Users/tunkeltesch/Desktop/Uni-Repos/Data-Engeneering/data/tripdata/*.csv does not exist\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
            "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
            "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
            "\tat scala.Option.getOrElse(Option.scala:201)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
            "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
            "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
            "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
            "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
            "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
            "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
            "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
            "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
            "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
            "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
            "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
            "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
            "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
            "\tat scala.util.Try$.apply(Try.scala:217)\n",
            "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
            "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
            "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
            "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
            "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
            "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
            "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
            "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
            "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
            "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n",
            "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "[Stage 0:====================================================>     (9 + 1) / 10]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Loaded 5,719,877 trips from CSV files\n",
            "  Data path: /Users/tunkeltesch/Desktop/Uni-Repos/Data-Engeneering/data/tripdata\n",
            "\n",
            "Schema:\n",
            "root\n",
            " |-- ride_id: string (nullable = true)\n",
            " |-- rideable_type: string (nullable = true)\n",
            " |-- started_at: timestamp (nullable = true)\n",
            " |-- ended_at: timestamp (nullable = true)\n",
            " |-- start_station_name: string (nullable = true)\n",
            " |-- start_station_id: string (nullable = true)\n",
            " |-- end_station_name: string (nullable = true)\n",
            " |-- end_station_id: string (nullable = true)\n",
            " |-- start_lat: double (nullable = true)\n",
            " |-- start_lng: double (nullable = true)\n",
            " |-- end_lat: double (nullable = true)\n",
            " |-- end_lng: double (nullable = true)\n",
            " |-- member_casual: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Define schema for trip data\n",
        "ride_schema = StructType([\n",
        "    StructField(\"ride_id\", StringType(), True),\n",
        "    StructField(\"rideable_type\", StringType(), True),\n",
        "    StructField(\"started_at\", TimestampType(), True),\n",
        "    StructField(\"ended_at\", TimestampType(), True),\n",
        "    StructField(\"start_station_name\", StringType(), True),\n",
        "    StructField(\"start_station_id\", StringType(), True),\n",
        "    StructField(\"end_station_name\", StringType(), True),\n",
        "    StructField(\"end_station_id\", StringType(), True),\n",
        "    StructField(\"start_lat\", DoubleType(), True),\n",
        "    StructField(\"start_lng\", DoubleType(), True),\n",
        "    StructField(\"end_lat\", DoubleType(), True),\n",
        "    StructField(\"end_lng\", DoubleType(), True),\n",
        "    StructField(\"member_casual\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Load CSV files\n",
        "data_path = PROJECT_ROOT / \"data\" / \"tripdata\"\n",
        "csv_pattern = str(data_path / \"*.csv\")\n",
        "\n",
        "df = (\n",
        "    spark.read\n",
        "    .option(\"header\", True)\n",
        "    .schema(ride_schema)\n",
        "    .csv(csv_pattern)\n",
        ")\n",
        "\n",
        "# Show data info\n",
        "trip_count = df.count()\n",
        "print(f\"âœ“ Loaded {trip_count:,} trips from CSV files\")\n",
        "print(f\"  Data path: {data_path}\")\n",
        "print(f\"\\nSchema:\")\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Sample Data\n",
        "\n",
        "View first few rows of trip data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+-------------+-------------------+----------------+------------------+--------------+----------------+\n",
            "|ride_id         |rideable_type|started_at         |start_station_id|start_station_name|end_station_id|end_station_name|\n",
            "+----------------+-------------+-------------------+----------------+------------------+--------------+----------------+\n",
            "|6F1682AC40EB6F71|electric_bike|2023-06-05 13:34:12|NULL            |NULL              |NULL          |NULL            |\n",
            "|622A1686D64948EB|electric_bike|2023-06-05 01:30:22|NULL            |NULL              |NULL          |NULL            |\n",
            "|3C88859D926253B4|electric_bike|2023-06-20 18:15:49|NULL            |NULL              |NULL          |NULL            |\n",
            "|EAD8A5E0259DEC88|electric_bike|2023-06-19 14:56:00|NULL            |NULL              |NULL          |NULL            |\n",
            "|5A36F21930D6A55C|electric_bike|2023-06-19 15:03:34|NULL            |NULL              |NULL          |NULL            |\n",
            "+----------------+-------------+-------------------+----------------+------------------+--------------+----------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# Display sample trips\n",
        "df.select(\n",
        "    \"ride_id\", \n",
        "    \"rideable_type\", \n",
        "    \"started_at\", \n",
        "    \"start_station_id\", \n",
        "    \"start_station_name\",\n",
        "    \"end_station_id\",\n",
        "    \"end_station_name\"\n",
        ").show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Data Statistics\n",
        "\n",
        "Quick statistics about the trip data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bike Types Distribution:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-------+\n",
            "|rideable_type|  count|\n",
            "+-------------+-------+\n",
            "|electric_bike|2945579|\n",
            "| classic_bike|2696011|\n",
            "|  docked_bike|  78287|\n",
            "+-------------+-------+\n",
            "\n",
            "\n",
            "Date Range:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+\n",
            "|earliest_trip      |latest_trip        |\n",
            "+-------------------+-------------------+\n",
            "|2023-01-01 00:01:58|2024-01-01 23:50:51|\n",
            "+-------------------+-------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 16:===================================================>     (9 + 1) / 10]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Unique start stations: 1517\n",
            "Unique end stations: 1521\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Count bike types\n",
        "print(\"Bike Types Distribution:\")\n",
        "df.groupBy(\"rideable_type\").count().orderBy(\"count\", ascending=False).show()\n",
        "\n",
        "# Date range\n",
        "print(\"\\nDate Range:\")\n",
        "df.select(\n",
        "    F.min(\"started_at\").alias(\"earliest_trip\"),\n",
        "    F.max(\"ended_at\").alias(\"latest_trip\")\n",
        ").show(truncate=False)\n",
        "\n",
        "# Count unique stations\n",
        "start_stations = df.select(\"start_station_id\").distinct().count()\n",
        "end_stations = df.select(\"end_station_id\").distinct().count()\n",
        "print(f\"\\nUnique start stations: {start_stations}\")\n",
        "print(f\"Unique end stations: {end_stations}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Data Quality - NULL Station Analysis\n",
        "\n",
        "Some trips (especially dockless electric bikes) don't have station information. Let's analyze this pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze NULL stations by bike type\n",
        "print(\"ðŸ” NULL STATION ANALYSIS\\n\")\n",
        "\n",
        "# Count trips with NULL start or end stations\n",
        "null_start = df.filter(F.col(\"start_station_id\").isNull()).count()\n",
        "null_end = df.filter(F.col(\"end_station_id\").isNull()).count()\n",
        "null_both = df.filter(F.col(\"start_station_id\").isNull() | F.col(\"end_station_id\").isNull()).count()\n",
        "\n",
        "print(f\"Trips with NULL start station: {null_start:,} ({null_start/trip_count*100:.1f}%)\")\n",
        "print(f\"Trips with NULL end station: {null_end:,} ({null_end/trip_count*100:.1f}%)\")\n",
        "print(f\"Trips with any NULL station: {null_both:,} ({null_both/trip_count*100:.1f}%)\")\n",
        "\n",
        "# Analyze by bike type\n",
        "print(\"\\nðŸ“Š NULL Stations by Bike Type:\")\n",
        "df.groupBy(\"rideable_type\").agg(\n",
        "    F.count(\"*\").alias(\"total_trips\"),\n",
        "    F.sum(F.when(F.col(\"start_station_id\").isNull(), 1).otherwise(0)).alias(\"null_start\"),\n",
        "    F.sum(F.when(F.col(\"end_station_id\").isNull(), 1).otherwise(0)).alias(\"null_end\")\n",
        ").withColumn(\n",
        "    \"null_start_pct\",\n",
        "    (F.col(\"null_start\") / F.col(\"total_trips\") * 100)\n",
        ").withColumn(\n",
        "    \"null_end_pct\", \n",
        "    (F.col(\"null_end\") / F.col(\"total_trips\") * 100)\n",
        ").show()\n",
        "\n",
        "# Count usable trips for station analysis\n",
        "usable_trips = df.filter(\n",
        "    F.col(\"start_station_id\").isNotNull() & F.col(\"end_station_id\").isNotNull()\n",
        ").count()\n",
        "\n",
        "print(f\"\\nâœ“ Usable trips for station analysis: {usable_trips:,} ({usable_trips/trip_count*100:.1f}%)\")\n",
        "print(f\"âœ— Excluded (dockless/NULL): {trip_count - usable_trips:,} ({(trip_count - usable_trips)/trip_count*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nðŸ’¡ NOTE: Electric bikes are often 'dockless' - users can pick up/drop off\")\n",
        "print(\"   anywhere, not just at stations. These trips are excluded from our\")\n",
        "print(\"   station-based rebalancing analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ETL Pipeline - Extract Stations\n",
        "\n",
        "Extract unique station catalog from trip data (filters out NULL/dockless trips).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Extracted 1535 unique stations\n",
            "\n",
            "Sample stations:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 28:=====================================================>  (19 + 1) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------------------------------------------+\n",
            "|station_id|station_name                                  |\n",
            "+----------+----------------------------------------------+\n",
            "|021320    |MTV Hubbard St                                |\n",
            "|1011      |Public Rack - Fullerton Ave & Narragansett Ave|\n",
            "|1012      |Public Rack - Langley Ave & 49th St           |\n",
            "|1013      |Public Rack - Pulaski Rd & Lake St            |\n",
            "|1015      |Public Rack - Peterson Ave & Drake Ave        |\n",
            "|1016      |Public Rack - Peterson Ave & Bernard Ave      |\n",
            "|1017      |Public Rack - Foster Ave & Drake Ave          |\n",
            "|1018      |Public Rack - Kostner Ave & Wrightwood Ave    |\n",
            "|1019      |Public Rack - Kostner Ave & Diversey Ave      |\n",
            "|1020      |Public Rack - Hamlin Ave & Madison St         |\n",
            "+----------+----------------------------------------------+\n",
            "only showing top 10 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Extract start stations\n",
        "start_stations = df.select(\n",
        "    F.col(\"start_station_id\").alias(\"station_id\"),\n",
        "    F.col(\"start_station_name\").alias(\"station_name\")\n",
        ")\n",
        "\n",
        "# Extract end stations\n",
        "end_stations = df.select(\n",
        "    F.col(\"end_station_id\").alias(\"station_id\"),\n",
        "    F.col(\"end_station_name\").alias(\"station_name\")\n",
        ")\n",
        "\n",
        "# Union and deduplicate\n",
        "stations_df = (\n",
        "    start_stations.union(end_stations)\n",
        "    .filter(F.col(\"station_id\").isNotNull())\n",
        "    .filter(F.col(\"station_name\").isNotNull())\n",
        "    .dropDuplicates([\"station_id\"])\n",
        "    .orderBy(\"station_id\")\n",
        ")\n",
        "\n",
        "station_count = stations_df.count()\n",
        "print(f\"âœ“ Extracted {station_count} unique stations\")\n",
        "print(\"\\nSample stations:\")\n",
        "stations_df.show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ETL Pipeline - Transform to Hourly Deltas\n",
        "\n",
        "Transform trips into hourly station balance changes:\n",
        "- Departure (start): -1 bike from station\n",
        "- Arrival (end): +1 bike to station\n",
        "- Aggregate by (hour, station_id)\n",
        "- **Note**: Trips with NULL stations are automatically filtered out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Generated 2,394,066 hourly events\n",
            "\n",
            "Sample hourly deltas:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 39:>                                                         (0 + 4) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----------+-----------+\n",
            "|               hour|station_id|delta_total|\n",
            "+-------------------+----------+-----------+\n",
            "|2023-01-01 00:00:00|     13006|         -2|\n",
            "|2023-01-01 00:00:00|     13008|         -1|\n",
            "|2023-01-01 00:00:00|     13016|          1|\n",
            "|2023-01-01 00:00:00|     13021|         -1|\n",
            "|2023-01-01 00:00:00|     13022|        -19|\n",
            "|2023-01-01 00:00:00|     13034|         -3|\n",
            "|2023-01-01 00:00:00|     13042|         -1|\n",
            "|2023-01-01 00:00:00|     13056|          1|\n",
            "|2023-01-01 00:00:00|     13061|         -1|\n",
            "|2023-01-01 00:00:00|     13063|          3|\n",
            "+-------------------+----------+-----------+\n",
            "only showing top 10 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Floor timestamps to hour\n",
        "df_with_hours = df.withColumn(\n",
        "    \"start_hour\",\n",
        "    F.date_format(\n",
        "        F.date_trunc(\"hour\", F.col(\"started_at\")),\n",
        "        \"yyyy-MM-dd HH:00:00\"\n",
        "    )\n",
        ").withColumn(\n",
        "    \"end_hour\",\n",
        "    F.date_format(\n",
        "        F.date_trunc(\"hour\", F.col(\"ended_at\")),\n",
        "        \"yyyy-MM-dd HH:00:00\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create departure events (delta = -1)\n",
        "departures = (\n",
        "    df_with_hours\n",
        "    .filter(F.col(\"start_station_id\").isNotNull())\n",
        "    .select(\n",
        "        F.col(\"start_hour\").alias(\"hour\"),\n",
        "        F.col(\"start_station_id\").alias(\"station_id\"),\n",
        "        F.lit(-1).alias(\"delta\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create arrival events (delta = +1)\n",
        "arrivals = (\n",
        "    df_with_hours\n",
        "    .filter(F.col(\"end_station_id\").isNotNull())\n",
        "    .select(\n",
        "        F.col(\"end_hour\").alias(\"hour\"),\n",
        "        F.col(\"end_station_id\").alias(\"station_id\"),\n",
        "        F.lit(1).alias(\"delta\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Union and aggregate\n",
        "hourly_deltas_df = (\n",
        "    departures.union(arrivals)\n",
        "    .groupBy(\"hour\", \"station_id\")\n",
        "    .agg(F.sum(\"delta\").alias(\"delta_total\"))\n",
        "    .orderBy(\"hour\", \"station_id\")\n",
        ")\n",
        "\n",
        "hourly_event_count = hourly_deltas_df.count()\n",
        "print(f\"âœ“ Generated {hourly_event_count:,} hourly events\")\n",
        "print(\"\\nSample hourly deltas:\")\n",
        "hourly_deltas_df.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Load Data into SQLite Database\n",
        "\n",
        "Write extracted stations and hourly events to SQLite using repository pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Converted 1535 stations to list\n",
            "âœ“ Converted 2,394,066 hourly events to list\n"
          ]
        }
      ],
      "source": [
        "# Convert Spark DataFrames to Python lists for repository\n",
        "stations_list = [\n",
        "    {\"id\": row.station_id, \"name\": row.station_name}\n",
        "    for row in stations_df.collect()\n",
        "]\n",
        "\n",
        "hourly_events_list = [\n",
        "    {\n",
        "        \"hour\": row.hour,\n",
        "        \"station_id\": row.station_id,\n",
        "        \"delta_total\": int(row.delta_total)\n",
        "    }\n",
        "    for row in hourly_deltas_df.collect()\n",
        "]\n",
        "\n",
        "print(f\"âœ“ Converted {len(stations_list)} stations to list\")\n",
        "print(f\"âœ“ Converted {len(hourly_events_list):,} hourly events to list\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing to database...\n",
            "âœ“ Upserted 1535 stations to database\n",
            "âœ“ Upserted 2,394,066 hourly events to database\n",
            "\n",
            "âœ“ Database updated successfully at: /Users/tunkeltesch/Desktop/Uni-Repos/Data-Engeneering/data/app.db\n"
          ]
        }
      ],
      "source": [
        "# Write to SQLite database using repositories\n",
        "print(\"Writing to database...\")\n",
        "\n",
        "with db.get_connection() as conn:\n",
        "    # Write stations\n",
        "    station_repo = StationRepository(conn)\n",
        "    stations_written = station_repo.upsert_stations_batch(stations_list)\n",
        "    print(f\"âœ“ Upserted {stations_written} stations to database\")\n",
        "    \n",
        "    # Write hourly events\n",
        "    events_repo = EventsHourlyRepository(conn)\n",
        "    events_written = events_repo.upsert_hourly_events_batch(hourly_events_list)\n",
        "    print(f\"âœ“ Upserted {events_written:,} hourly events to database\")\n",
        "\n",
        "print(f\"\\nâœ“ Database updated successfully at: {DB_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Data Analysis & Visualization\n",
        "\n",
        "Query the database to analyze station balances and identify imbalances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import visualization libraries\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmdates\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Configure plot style\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "# Import visualization libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# Configure plot style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Overall Station Balance Analysis\n",
        "\n",
        "Calculate net balance changes for all stations across the entire time period.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query overall balance for all stations\n",
        "with db.get_connection() as conn:\n",
        "    query = \"\"\"\n",
        "        SELECT \n",
        "            s.id as station_id,\n",
        "            s.name as station_name,\n",
        "            COALESCE(SUM(e.delta_total), 0) as net_change,\n",
        "            ? + COALESCE(SUM(e.delta_total), 0) as final_balance\n",
        "        FROM station s\n",
        "        LEFT JOIN events_hourly e ON s.id = e.station_id\n",
        "        WHERE s.is_active = 1\n",
        "        GROUP BY s.id, s.name\n",
        "        ORDER BY net_change DESC\n",
        "    \"\"\"\n",
        "    balance_df = pd.read_sql_query(query, conn, params=(INITIAL_BALANCE,))\n",
        "\n",
        "print(f\"âœ“ Loaded balances for {len(balance_df)} stations\\n\")\n",
        "print(\"Top 10 stations with most bikes accumulated:\")\n",
        "print(balance_df.head(10)[['station_name', 'net_change', 'final_balance']].to_string(index=False))\n",
        "print(\"\\nTop 10 stations with most bikes depleted:\")\n",
        "print(balance_df.tail(10)[['station_name', 'net_change', 'final_balance']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Balance Distribution Visualization\n",
        "\n",
        "Visualize the distribution of final balances across all stations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot balance distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Histogram of final balances\n",
        "axes[0].hist(balance_df['final_balance'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].axvline(INITIAL_BALANCE, color='red', linestyle='--', linewidth=2, label=f'Initial Balance ({INITIAL_BALANCE})')\n",
        "axes[0].set_xlabel('Final Balance (bikes)', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Stations', fontsize=12)\n",
        "axes[0].set_title('Distribution of Final Station Balances', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Bar chart of top 20 stations by net change\n",
        "top_20 = pd.concat([balance_df.head(10), balance_df.tail(10)]).sort_values('net_change', ascending=False)\n",
        "colors = ['green' if x > 0 else 'red' for x in top_20['net_change']]\n",
        "axes[1].barh(range(len(top_20)), top_20['net_change'], color=colors, alpha=0.7)\n",
        "axes[1].set_yticks(range(len(top_20)))\n",
        "axes[1].set_yticklabels([name[:30] for name in top_20['station_name']], fontsize=9)\n",
        "axes[1].set_xlabel('Net Change (bikes)', fontsize=12)\n",
        "axes[1].set_title('Top 10 Surplus & Deficit Stations', fontsize=14, fontweight='bold')\n",
        "axes[1].axvline(0, color='black', linewidth=1)\n",
        "axes[1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(f\"\\nBalance Statistics:\")\n",
        "print(f\"  Mean final balance: {balance_df['final_balance'].mean():.1f} bikes\")\n",
        "print(f\"  Median final balance: {balance_df['final_balance'].median():.1f} bikes\")\n",
        "print(f\"  Std deviation: {balance_df['final_balance'].std():.1f} bikes\")\n",
        "print(f\"  Min: {balance_df['final_balance'].min():.0f} bikes\")\n",
        "print(f\"  Max: {balance_df['final_balance'].max():.0f} bikes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Time-Series Analysis for Specific Station\n",
        "\n",
        "Track balance changes over time for the station with highest deficit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick the station with highest deficit\n",
        "target_station = balance_df.tail(1).iloc[0]\n",
        "station_id = target_station['station_id']\n",
        "station_name = target_station['station_name']\n",
        "\n",
        "print(f\"Analyzing: {station_name} (ID: {station_id})\")\n",
        "print(f\"  Net change: {target_station['net_change']:.0f} bikes\")\n",
        "print(f\"  Final balance: {target_station['final_balance']:.0f} bikes\\n\")\n",
        "\n",
        "# Query hourly events for this station\n",
        "with db.get_connection() as conn:\n",
        "    query = \"\"\"\n",
        "        SELECT hour, delta_total\n",
        "        FROM events_hourly\n",
        "        WHERE station_id = ?\n",
        "        ORDER BY hour\n",
        "    \"\"\"\n",
        "    station_events = pd.read_sql_query(query, conn, params=(station_id,))\n",
        "\n",
        "# Convert hour to datetime and calculate cumulative balance\n",
        "station_events['hour'] = pd.to_datetime(station_events['hour'])\n",
        "station_events['cumulative_balance'] = INITIAL_BALANCE + station_events['delta_total'].cumsum()\n",
        "\n",
        "print(f\"âœ“ Loaded {len(station_events)} hourly events\")\n",
        "print(f\"\\nTime range: {station_events['hour'].min()} to {station_events['hour'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot time series\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "\n",
        "# Hourly deltas\n",
        "axes[0].bar(station_events['hour'], station_events['delta_total'], width=0.03, alpha=0.7, color='steelblue')\n",
        "axes[0].axhline(0, color='black', linewidth=1)\n",
        "axes[0].set_xlabel('Time', fontsize=12)\n",
        "axes[0].set_ylabel('Hourly Delta (bikes)', fontsize=12)\n",
        "axes[0].set_title(f'Hourly Balance Changes: {station_name}', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Cumulative balance\n",
        "axes[1].plot(station_events['hour'], station_events['cumulative_balance'], linewidth=2, color='darkgreen')\n",
        "axes[1].axhline(INITIAL_BALANCE, color='red', linestyle='--', linewidth=2, label=f'Initial Balance ({INITIAL_BALANCE})')\n",
        "axes[1].axhline(5, color='orange', linestyle='--', linewidth=1, label='Low Threshold (5)', alpha=0.7)\n",
        "axes[1].axhline(30, color='purple', linestyle='--', linewidth=1, label='High Threshold (30)', alpha=0.7)\n",
        "axes[1].set_xlabel('Time', fontsize=12)\n",
        "axes[1].set_ylabel('Cumulative Balance (bikes)', fontsize=12)\n",
        "axes[1].set_title(f'Cumulative Balance Over Time: {station_name}', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.4 Critical Stations Detection\n",
        "\n",
        "Identify stations that need immediate rebalancing attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import thresholds from config\n",
        "from config.database import BALANCE_LOW_THRESHOLD, BALANCE_HIGH_THRESHOLD\n",
        "\n",
        "# Identify critical stations\n",
        "low_balance_stations = balance_df[balance_df['final_balance'] <= BALANCE_LOW_THRESHOLD]\n",
        "high_balance_stations = balance_df[balance_df['final_balance'] >= BALANCE_HIGH_THRESHOLD]\n",
        "\n",
        "print(\"ðŸš¨ CRITICAL STATIONS - LOW BALANCE (Need bikes added)\")\n",
        "print(f\"   {len(low_balance_stations)} stations with â‰¤ {BALANCE_LOW_THRESHOLD} bikes\\n\")\n",
        "if len(low_balance_stations) > 0:\n",
        "    print(low_balance_stations[['station_name', 'final_balance']].head(15).to_string(index=False))\n",
        "else:\n",
        "    print(\"   None found âœ“\")\n",
        "\n",
        "print(f\"\\n\\nðŸš¨ CRITICAL STATIONS - HIGH BALANCE (Need bikes removed)\")\n",
        "print(f\"   {len(high_balance_stations)} stations with â‰¥ {BALANCE_HIGH_THRESHOLD} bikes\\n\")\n",
        "if len(high_balance_stations) > 0:\n",
        "    print(high_balance_stations[['station_name', 'final_balance']].head(15).to_string(index=False))\n",
        "else:\n",
        "    print(\"   None found âœ“\")\n",
        "\n",
        "# Calculate rebalancing needs\n",
        "total_deficit = low_balance_stations['net_change'].sum() if len(low_balance_stations) > 0 else 0\n",
        "total_surplus = high_balance_stations['net_change'].sum() if len(high_balance_stations) > 0 else 0\n",
        "\n",
        "print(f\"\\n\\nðŸ“Š REBALANCING SUMMARY:\")\n",
        "print(f\"   Total deficit: {abs(total_deficit):.0f} bikes need to be added\")\n",
        "print(f\"   Total surplus: {total_surplus:.0f} bikes need to be removed\")\n",
        "print(f\"   Net imbalance: {(total_surplus + total_deficit):.0f} bikes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Pipeline Summary & Cleanup\n",
        "\n",
        "Summary of the complete pipeline execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session\n",
        "spark.stop()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"  DIVVY BIKE SHARING PIPELINE - EXECUTION COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nðŸ“‹ PIPELINE SUMMARY:\")\n",
        "print(f\"   âœ“ Processed: {trip_count:,} trips\")\n",
        "print(f\"   âœ“ Extracted: {station_count} unique stations\")\n",
        "print(f\"   âœ“ Generated: {hourly_event_count:,} hourly events\")\n",
        "print(f\"   âœ“ Database: {DB_PATH}\")\n",
        "print(f\"   âœ“ Timezone: {TIMEZONE}\")\n",
        "\n",
        "print(\"\\nðŸ“Š ANALYSIS RESULTS:\")\n",
        "print(f\"   â€¢ Stations analyzed: {len(balance_df)}\")\n",
        "print(f\"   â€¢ Critical low balance: {len(low_balance_stations)}\")\n",
        "print(f\"   â€¢ Critical high balance: {len(high_balance_stations)}\")\n",
        "print(f\"   â€¢ Mean final balance: {balance_df['final_balance'].mean():.1f} bikes\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ NEXT STEPS:\")\n",
        "print(\"   1. Launch Streamlit dashboard:\")\n",
        "print(\"      streamlit run src/streamlit_test.py\")\n",
        "print(\"   2. Explore station-specific patterns\")\n",
        "print(\"   3. Plan rebalancing operations\")\n",
        "print(\"   4. Monitor real-time balance changes\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates a complete end-to-end data engineering pipeline for bike-sharing analytics:\n",
        "\n",
        "### Key Components\n",
        "1. **Data Ingestion**: Read CSV trip histories using PySpark\n",
        "2. **Data Transformation**: Extract stations and aggregate to hourly deltas\n",
        "3. **Data Storage**: Persist to SQLite database using repository pattern\n",
        "4. **Data Analysis**: Query and visualize balance patterns\n",
        "5. **Operational Insights**: Identify critical stations requiring rebalancing\n",
        "\n",
        "### Architecture Highlights\n",
        "- **Separation of Concerns**: ETL (Spark) â†’ Storage (SQLite) â†’ Visualization (Pandas/Matplotlib)\n",
        "- **Timezone Handling**: All hourly events in Chicago local time (America/Chicago)\n",
        "- **Repository Pattern**: Clean abstraction for database operations\n",
        "- **Upsert Logic**: Idempotent writes for reprocessing data\n",
        "\n",
        "### Business Value\n",
        "- **Proactive Operations**: Identify imbalances before they impact customers\n",
        "- **Data-Driven Decisions**: Historical patterns inform rebalancing strategy\n",
        "- **Scalable Foundation**: Ready for migration to PostgreSQL/Kubernetes\n",
        "\n",
        "### Team Contributions\n",
        "- **Tun**: SQLite database layer and repository pattern\n",
        "- **Sebi**: PySpark ETL pipeline and batch processing\n",
        "- **Felix**: Streamlit dashboard and visualization\n",
        "- **Oli**: Docker containerization and deployment\n",
        "\n",
        "---\n",
        "\n",
        "**Project Repository**: [Data-Engeneering](https://github.com/FelixHuefner0/Data-Engeneering)  \n",
        "**Data Source**: [Divvy Trip Data](https://divvybikes.com/system-data)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
